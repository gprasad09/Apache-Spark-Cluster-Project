{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb7da687",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /usr/lib/python3/dist-packages/IPython/utils/py3compat.py:168 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_655/399551480.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create a SparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"First App\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    193\u001b[0m             )\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             self._do_init(\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    431\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /usr/lib/python3/dist-packages/IPython/utils/py3compat.py:168 "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"First App\")\n",
    "\n",
    "\n",
    "# Create an RDD (Resilient Distributed Dataset)\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Perform operations on the RDD\n",
    "rdd2 = rdd.map(lambda x: x * 2)\n",
    "rdd3 = rdd2.filter(lambda x: x > 5)\n",
    "\n",
    "# Print the final result\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29cc8d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= \"text.txt\"\n",
    "data = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964c6f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66787210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, How are you ?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd06273c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello, How are you ?',\n",
       " 'This is a test file which will be used as input file.',\n",
       " 'We will count words or lines in this file later.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82bcb3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sp-manager:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b973db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"state_info.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de98d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= spark.read.csv(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c511ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='State', _c1='StateCode', _c2='Capital', _c3='LargestCity', _c4='PopulationEst2015', _c5='HouseSeats', _c6='StateHood'),\n",
       " Row(_c0='Alabama', _c1='AL', _c2='Montgomery', _c3='Birmingham', _c4='4858979', _c5='7', _c6='1819-12-14')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a35eaafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= spark.read.load(path,format = \"csv\",header='true',inferSchema='true')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ff62e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-----------+-----------------+----------+-------------------+\n",
      "|     State|StateCode|    Capital|LargestCity|PopulationEst2015|HouseSeats|          StateHood|\n",
      "+----------+---------+-----------+-----------+-----------------+----------+-------------------+\n",
      "|   Alabama|       AL| Montgomery| Birmingham|          4858979|         7|1819-12-14 00:00:00|\n",
      "|    Alaska|       AK|     Juneau|  Anchorage|           738432|         1|1959-01-03 00:00:00|\n",
      "|   Arizona|       AZ|    Phoenix|    Phoenix|          6828065|         9|1912-02-14 00:00:00|\n",
      "|  Arkansas|       AR|Little Rock|Little Rock|          2978204|         4|1836-06-15 00:00:00|\n",
      "|California|       CA| Sacramento|Los Angeles|         39144818|        53|1850-09-09 00:00:00|\n",
      "+----------+---------+-----------+-----------+-----------------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90ead857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- StateCode: string (nullable = true)\n",
      " |-- Capital: string (nullable = true)\n",
      " |-- LargestCity: string (nullable = true)\n",
      " |-- PopulationEst2015: integer (nullable = true)\n",
      " |-- HouseSeats: integer (nullable = true)\n",
      " |-- StateHood: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6a6bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1= df.select(\"State\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea0d7f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "r1.write.saveAsTable(\"state_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae950473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%sql` not found.\n"
     ]
    }
   ],
   "source": [
    "%sql select * from state_tbl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e26be088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyspark.context.SparkContext, pyspark.sql.session.SparkSession)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc), type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40b84661",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql= \"select * from state_tbl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be3a4f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test= spark.sql(sql);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e4c8a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb952720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        State|\n",
      "+-------------+\n",
      "|         Utah|\n",
      "|       Hawaii|\n",
      "|    Minnesota|\n",
      "|         Ohio|\n",
      "|     Arkansas|\n",
      "|       Oregon|\n",
      "|        Texas|\n",
      "| North Dakota|\n",
      "| Pennsylvania|\n",
      "|  Connecticut|\n",
      "|     Nebraska|\n",
      "|      Vermont|\n",
      "|       Nevada|\n",
      "|   Washington|\n",
      "|     Illinois|\n",
      "|     Oklahoma|\n",
      "|     Delaware|\n",
      "|       Alaska|\n",
      "|   New Mexico|\n",
      "|West Virginia|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9932831a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unexpected item type: <class 'slice'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_621/1259702016.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1973\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1975\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected item type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unexpected item type: <class 'slice'>"
     ]
    }
   ],
   "source": [
    "for i in list(range(test.count())):\n",
    "    print(test[i:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7fbb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d226b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
